{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import gzip\n",
    "import spacy\n",
    "#import textacy\n",
    "%matplotlib inline\n",
    "np.random.seed(32113)\n",
    "import pickle\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#import xgboost as XGB\n",
    "import string\n",
    "parser = English()\n",
    "import data_prep_for_test_run as dp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading reviews and meta data\n",
    "path = './reviews_Video_Games.json.gz'\n",
    "meta_path = './meta_Video_Games.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before creating  a model:\n",
    "In order to create an accurate model, we want to filter reviews.  \n",
    "we filter review by looking at categories of product in meta data. \n",
    "  \n",
    "Below are codes to check products associated with review data.\n",
    "It displays types of categories and number of 'Home & Kitchen' products that are associated with each category.  \n",
    "As you can see, there are many 'Home & kitchen' products  \n",
    "that are associated with categories that are unrelated to Home & Kitchen. \n",
    "By looking at this list, we can decide how to filter products.  \n",
    "  \n",
    "In this case, I would filter out categories where total number is less than 2000.  \n",
    "If you want to strictly work on Home & kitchen products, I will filter out categories less than 10000.  \n",
    "\n",
    "Once we merge review data and meta data, we will filter out reviews by these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code if you do not know if products belong to unrelated categories\n",
    "df_test = dp.Data_prep1(path)\n",
    "meta = dp.getDF(meta_path)\n",
    "meta = dp.add_meta_info(df_test,meta,'Video Games',50,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging data, Cleaning Meta data  \n",
    "the function **df_prep1** will do three things for you:  \n",
    "1. Cleaning meta data for you (gets rid of Nan values)  \n",
    "2. Add features created from Meta data (categories, sales ranking within the categories and price)   \n",
    "3. Filter out categories that are not associated with the data topics  \n",
    "4. Merge meta data with review datas and output pandas dataframe \n",
    "5. reassign values of 'helpful' features into 2 values and calculate percentage of positive votes on a review.  \n",
    "  \n",
    "inputs:  \n",
    "**dp.df_prep1( a , b , c , d )**  \n",
    "a = path of your review data (json.gz format)  \n",
    "b = path of your meta data (json.gz format)  \n",
    "c = topic keys: category names that every product in your dataset is associated with.  \n",
    "for example: 'Video Games', 'Home &amp; Kitchen' etc.  \n",
    "d = filtering out categoiry if the total number of product associated with that category is less than this number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average price for Electronics is $33.51\n",
      "average price for Video Games is $61.456088411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_prep_for_test_run.py:891: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df_topic['price'] = df_topic['price'].fillna(average_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average price for Software is $23.8273076923\n",
      "--- 312.00524807 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df= dp.df_prep1(path, meta_path,'Video Games',50)\n",
    "# should spit out average price for each category.  \n",
    "# these price will be filling Nan under price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test dataset:\n",
    "In NLP prediction model, it is safe to run train and test dataset before running model building functions.  \n",
    "Here, we are randomly selecting 85% of data as a train dataset.  \n",
    "and assign rest of the data as test dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = df.index\n",
    "random_ind = np.random.choice(list(ind), int(len(df)*.85), replace=False)\n",
    "df_train = df.loc[list(random_ind)]\n",
    "test_ind = [i for i in ind if i not in random_ind]\n",
    "df_test = df.loc[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## if you want to save test and training dataframe...\n",
    "#df_test.to_pickle(\"./HomeandKitchen_df_test_15percent.pkl\")\n",
    "#df_train.to_pickle(\"./HomeandKitchen_df_train_85percent.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf model\n",
    "**df_prep_train1** function creates tfidf matrix and tfidf sklearn model.  \n",
    "It also saves tfidf matrix and tfidf skearn model as a pickle file in case something happens later.  \n",
    "  \n",
    "inputs:\n",
    "**df_prep_train1( a , b , c(optional) )**  \n",
    "a = your training dataframe created above  \n",
    "b = filename/keyword. this string will be used to name tfidf matrix and tfidf sklearn model pickle file.  \n",
    "c = a list containing parameters of TfidfVectorizer sklearn class. this is optional.  \n",
    "default values for this input is:  tfidf_set = [2,0.95,10000,'l2']  \n",
    "  \n",
    "where TfidfVectorizer(stop_words=STOPLIST, tokenizer=lemma, min_df=tfidf_set[0], max_df=tfidf_set[1], max_features =tfidf_set[2], norm=tfidf_set[3])  \n",
    "  \n",
    "visit Sklearn web for more details.     \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 121.213349819 seconds ---\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix, tfidf_model  = dp.df_prep_train1(df_train,'Video Games')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before running NMF, check your elbow!\n",
    "the function **NMF_elbow** will give you stats about reconstruction error for different K component.  \n",
    "you want to chose right K where slope changes significantly.  \n",
    "Since the data we are using does not have obvious elbow,  \n",
    "I added error difference between K and error of each K.  \n",
    "\n",
    "inputs:  \n",
    "**NMF_elbow( a , b )**  \n",
    "a = your tfidf matrix  \n",
    "b = range of K (list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated in 11.8010890484 seconds \n",
      "\n",
      "\n",
      "error difference\n",
      "0.275268431994\n",
      "0.187867672658\n",
      "0.154415273565\n",
      "0.133671663843\n",
      "0.138395448223\n",
      "0.112929126418\n",
      "0.120690769262\n",
      "0.105908771244\n",
      "0.0912280233686\n",
      "0.102247955315\n",
      "\n",
      "\n",
      "error k = 1: 58.1219805649\n",
      "error k = 2: 57.8467121329\n",
      "error k = 3: 57.6588444602\n",
      "error k = 4: 57.5044291867\n",
      "error k = 5: 57.3707575228\n",
      "error k = 6: 57.2323620746\n",
      "error k = 7: 57.1194329482\n",
      "error k = 8: 56.9987421789\n",
      "error k = 9: 56.8928334077\n",
      "error k = 10: 56.8016053843\n",
      "error k = 11: 56.699357429\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226cd2f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp.NMF_elbow(tfidf_matrix, range(1,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding NMF results and tfidf results onto training dataset\n",
    "**df_prep_train2** function does lots of things here.\n",
    "1. calculate NMF from the tfidf matrix that you created from df_prep_train1\n",
    "2. add NMF results as features (predict percentage values for each NMF topics)\n",
    "3. print outs top 20 words that are highly associated with each NMF topics.\n",
    "4. add tfidf results to training dataset  \n",
    "(you can select how many words you want to add to the data frame by choosing values for max_feat (e))  \n",
    "5. saves resulting training dataframe, NMF model and tfidf feature model as pickle files\n",
    "\n",
    "inputs:  \n",
    "**df_prep_train2**( a , b , c , d , e (opt) , f (opt), g (opt))  \n",
    "a = training dataframe.  \n",
    "b = filename/keyword. this string will be used to name NMF model and tfidf feature model pickle file.  \n",
    "c = tfidf matrix.\n",
    "d = tfidf model.  \n",
    "e = max feature (optional). default value = 1000. \n",
    "will create tfidf model with top 1000 (default) words   \n",
    "and create features of these top 1000 words in your training dataframe.  \n",
    "f = NMF parameters (optional). default values NMF_set = [6,'cd',0.1,0.5].  \n",
    "where NMF(n_components=NMF_set[0], solver=NMF_set[1], random_state=32113, alpha=NMF_set[2], l1_ratio=NMF_set[3])  \n",
    "visit Sklearn web for more details.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html  \n",
    "\n",
    "g = tfidf parameters (optional). default values = [2,0.95,10000,'l2'].  \n",
    "  \n",
    "where TfidfVectorizer(stop_words=STOPLIST, tokenizer=lemma, min_df=tfidf_set[0], max_df=tfidf_set[1], max_features =tfidf_set[2], norm=tfidf_set[3])  \n",
    "visit Sklearn web for more details.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  \n",
    "(3rd value will be replaced by max feature (e))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "game play like good make time character really player new fun thing look story feel say level great think want\n",
      "Topic #1:\n",
      "xbox ps3 360 gb drive console microsoft sony buy hard cable kinect disc ray blu download playstation ps2 slim usb\n",
      "Topic #2:\n",
      "wii u nintendo remote sport game motion nunchuck buy control zelda use u. gamecube fit tv console kid board bundle\n",
      "Topic #3:\n",
      "drm game ea steam install buy server play purchase pay internet computer software securom customer amazon pc download product money\n",
      "Topic #4:\n",
      "exercise workout fitness board balance weight fit yoga routine trainer cardio kinect wii program gym use training active calorie body\n",
      "Topic #5:\n",
      "mario super nintendo ds kart luigi bro game galaxy 3d 3ds 64 new bowser brother bundle classic princess peach mini\n",
      "Topic #6:\n",
      "sim sims expansion new pack city ea house 3 2 career family object maxis pet create like build thing trait\n",
      "Topic #7:\n",
      "controller charge button battery use usb plug wireless work driver rumble pad dualshock analog stick pc device cable remote ps3\n",
      "Topic #8:\n",
      "dance song guitar band mode zumba drum ddr sing play fun rock kid hero music dancing kinect sweat dancer version\n",
      "Topic #9:\n",
      "ps4 sony console xbox amazon box good gen blue light replacement hdmi graphic review star doa buy look work entertainment\n",
      "--end--\n",
      "--- 118.188269854 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_train_new,NMF_model,TFIDF_feat_model = dp.df_prep_train2(df_train, 'Video Games', tfidf_matrix, tfidf_model,1000, [10,'cd',0.1,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## in case you need to load each model, you can load models here\n",
    "\n",
    "## df_prep1/train-test split result\n",
    "#df_train = pd.read_pickle('./HomeandKitchen_df_train_85percent.pkl')\n",
    "#df_test = pd.read_pickle('./HomeandKitchen_df_test_15percent.pkl')\n",
    "\n",
    "## df_prep_train1 results\n",
    "# with open(r\"HomeAndKitchen_tfidf_matrix.pickle\", \"rb\") as input_file:\n",
    "#     tfidf_matrix = pickle.load(input_file)\n",
    "# with open(r\"HomeAndKitchen_tfidf_model.pickle\", \"rb\") as input_file:\n",
    "#     tfidf_model= pickle.load(input_file)\n",
    "\n",
    "## df_prep_train2 results\n",
    "df_train_new = pd.read_pickle('./preped_HomeAndKitchen_max_feature1000.pkl')\n",
    "# with open('HomeAndKitchen_NMF_model.pickle', \"rb\") as input_file:\n",
    "#     NMF_model = pickle.load(input_file)\n",
    "# with open(r\"HomeAndKitchen_tfidf_feat_model.pickle\", \"rb\") as input_file:\n",
    "#     TFIDF_feat_model= pickle.load(input_file)\n",
    "df_test_new = pd.read_pickle('./preped_HomeAndKitchen_df_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test dataset\n",
    "\n",
    "Now we want to prepare test dataset.  \n",
    "I simply used MNF and tfidf models of training dataset to transform test dataset.  \n",
    "\n",
    "\n",
    "**df_prep_test** function adds features for test data.  \n",
    "It uses transform method of NMF and tfidf models that we just trained with training dataset.  \n",
    "The function outputs test dataframe which contains same amount of features as training dataframe.  \n",
    "It also saves output dataframe as a pickle file.  \n",
    "\n",
    "inputs:  \n",
    "**df_prep_test**( a , b , c , d , e )  \n",
    "a = test dataframe we created previously. (read Split train and test dataset section above)  \n",
    "b = tfidf model trained with training dataframe.  \n",
    "c = NMF model that was trained with training dataframe.\n",
    "d = second tfidf model that is used to create tfidf term features.  \n",
    "e = filename/keywords used as a name of pickle file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          reviewerID        asin  \\\n",
      "32     APGAQX4VA5SAX  B0000296YH   \n",
      "1637   A74TA8X5YQ7NE  B001B5U80K   \n",
      "1811  A1SB9BNNGKNX2Z  B001O3066U   \n",
      "3735  A3NPSVM5H907RF  B009S4JZKK   \n",
      "1     A15C05TWM2J5PL  B00000DMB3   \n",
      "2      ACQDEUA5ECO7J  B00000DMB3   \n",
      "11    A3R6DWGDCLH7IY  B00000JRSB   \n",
      "13    A1046G79VW4NKN  B00000JRSB   \n",
      "14    A3OBL19DP3UR9H  B00000JRSB   \n",
      "19    A1AP90K9PG0YQ7  B00000K4KT   \n",
      "24    A2BO5HRT9HFWAG  B00001IVRD   \n",
      "37    A31BO23GECXU13  B00002NDRL   \n",
      "42    A2UTTRGMOVMR6U  B00002S6CC   \n",
      "45    A21X68W8SJA3T4  B00002S6CC   \n",
      "50    A2B5XH474FI20T  B00002SU5D   \n",
      "72    A35RDX6W4CINQ9  B00004T1M7   \n",
      "89    A3N1V8LIFT0RU6  B00004VP4M   \n",
      "91     AJSVA9Y3UVXAJ  B00004WKHO   \n",
      "96    A26BY28XSBPBNU  B00004YRPD   \n",
      "111    AQ40L2YGV7C42  B000059S8D   \n",
      "124   A2RRVDJN4VVQ5J  B00005LABK   \n",
      "126   A2W3H2FFKN330R  B00005LBHM   \n",
      "131   A2ES7IT00AP3F1  B00005MDZY   \n",
      "135    AQ40L2YGV7C42  B00005N6ZL   \n",
      "166   A3OU7B1W9Y3JBH  B00005R5PO   \n",
      "173    ABUWRGAW6CWP7  B00005TNI6   \n",
      "179   A3CFSQ5QMBU32V  B00005UWMC   \n",
      "184    AQD80XZCX40QF  B00005V9Q1   \n",
      "187    AVQ7Z71AB9OM8  B000062SSB   \n",
      "192   A15UXO3PDUOGJY  B00006663Z   \n",
      "...              ...         ...   \n",
      "514   A2KGO6LUQ42ZLS  B0002Y2XXQ   \n",
      "519   A3832HPAY5CMN7  B0002Y2XXQ   \n",
      "523   A1HWZ62CRDHAAO  B000300HJQ   \n",
      "524   A29DRVRIJ9U3F8  B000300HK0   \n",
      "528   A1O26YWZJ82P68  B00030GSJY   \n",
      "534   A3LY7OIIWTV14F  B00063BLG8   \n",
      "540    A6VS5QLJ3X0UX  B00068O278   \n",
      "546   A3SD4CFPERCANZ  B0006B98DI   \n",
      "556   A3V6Z4RCDGRC44  B0007GBBHI   \n",
      "561   A1P4F6HIXU6WYE  B0007PNEGA   \n",
      "571   A1QV8TEG5LVSQW  B0007XMZ8K   \n",
      "573   A33693RRRFK55Q  B0007ZD79E   \n",
      "575   A1XE2XBK7T46VR  B0008EZDDM   \n",
      "576   A3J5KL0JUZMBT3  B0008EZDDM   \n",
      "579   A16RMJ937TGWU9  B0008GLHLW   \n",
      "585   A1ELM0GEXEAM79  B0009A4EVM   \n",
      "595   A3V6Z4RCDGRC44  B0009I6S0Y   \n",
      "602    AEOAMS0G96E8B  B0009PQCSU   \n",
      "609    A9998E2ZCHZAG  B0009UBR3K   \n",
      "632   A230X6O24Y82LL  B0009VXBAQ   \n",
      "648   A3B20R4ZAH0Y2Y  B0009W8QB4   \n",
      "658   A1NHQH6ZI2PV3Z  B0009Z3K3U   \n",
      "659   A2YN1IQT2CWKCB  B0009Z3MGU   \n",
      "662   A1R2IXGESXQRWW  B0009Z3MQK   \n",
      "663   A1PX45H8Q9Q4SJ  B0009Z3MQK   \n",
      "671    A3ZC3AO36WQHD  B000A76ZNO   \n",
      "676   A3SHMFQU8LFU5Q  B000ANYFVM   \n",
      "678   A3W4D8XOGLWUN5  B000AQA9UA   \n",
      "692   A2PXOP8AOGXQCO  B000B5MV6A   \n",
      "704    A8PS6YTA2Y0UX  B000B6MLSC   \n",
      "\n",
      "                                             reviewText  overall  \\\n",
      "32    This is a great piece of work.  All articles, ...      4.0   \n",
      "1637  This game sure comes with some serious pedigre...      5.0   \n",
      "1811  I installed the software onto a new HP desktop...      5.0   \n",
      "3735  I ordered this simulator thinking that the pre...      1.0   \n",
      "1     ...  I purchased the game for him and he waite...      1.0   \n",
      "2     Seriously, this ranks up with some of the wors...      1.0   \n",
      "11    Look at most of the 5 star reviews, they basic...      1.0   \n",
      "13    Boy, let me tell you, this games is seriously ...      1.0   \n",
      "14    I dont like this game and have to say that alo...      1.0   \n",
      "19    Haven't had a person around to play Scrabble w...      5.0   \n",
      "24    This game is terrible, the graphics stink, the...      1.0   \n",
      "37    I was very disappointed because the game is no...      4.0   \n",
      "42    I would love to own this game just as much as ...      5.0   \n",
      "45    I was really excited about getting Who Wants t...      2.0   \n",
      "50    I have played a lot of Star Trek games and thi...      5.0   \n",
      "72    For those who don't have SimCity 3000 already,...      4.0   \n",
      "89    the expansion pack features 5 new civ's, maya,...      5.0   \n",
      "91    This game surpasses every other game made for ...      5.0   \n",
      "96    For his birthday, my son received this case al...      3.0   \n",
      "111   I've had this game for less than 24 hours so I...      5.0   \n",
      "124   I have played the first jedi knight, and I tho...      1.0   \n",
      "126   1.The Best thing about this game is the new ba...      5.0   \n",
      "131   Although this game is not being released in th...      5.0   \n",
      "135   I've had the game since the day it came out. W...      5.0   \n",
      "166   I am a 40-something gamer and as an adult game...      5.0   \n",
      "173   My first impression of the game was that it wo...      5.0   \n",
      "179   If you are going on a trip or your TV is in th...      5.0   \n",
      "184   \"This Battle.net Beta includes all four of the...      5.0   \n",
      "187   Pros:1. metal and sturdy (except throttle leve...      2.0   \n",
      "192   Chessmaster 9000 has a lot of things going for...      3.0   \n",
      "...                                                 ...      ...   \n",
      "514   WORST GAME EVER MADE WOOOOOOOOOORST DONT BUY I...      1.0   \n",
      "519   Usually when a game is rated &quot;E&quot; for...      1.0   \n",
      "523   I was going to resist getting Zoo Tycoon 2, ha...      5.0   \n",
      "524   I've waited with baited breath for Zoo Tycoon ...      5.0   \n",
      "528   As a previous owner of the X45 flight control ...      5.0   \n",
      "534   The price as the time writing this review is $...      4.0   \n",
      "540   This game is so much fun it is difficult to ex...      5.0   \n",
      "546   This game is one of the titles that prompted m...      4.0   \n",
      "556   I'm a huge fan of Fable, Populous and Black & ...      5.0   \n",
      "561   From the minute I met Maya, my new personal tr...      5.0   \n",
      "571   One day, I recalled fond memories of playing H...      4.0   \n",
      "573   Look, San Andreas is a great game, no question...      2.0   \n",
      "575   Quite possibly the very best idea for a MMORPG...      1.0   \n",
      "576   For those that were back on Everquest during t...      1.0   \n",
      "579   This game/sim is great fun.  It has a truly dy...      5.0   \n",
      "585   A sequel that everyone's been waiting for, for...      5.0   \n",
      "595   We really love the Ratchet and Clank series of...      4.0   \n",
      "602   Playing Dungeon Siege 2 is like playing Dungeo...      4.0   \n",
      "609   We are americans living in japan and currently...      5.0   \n",
      "632   The Wii is without a doubt the worst of the th...      1.0   \n",
      "648   My 7 year old daughter and I have been searchi...      5.0   \n",
      "658   + Now has 50 songs+ You can design and create ...      5.0   \n",
      "659   First of all, I think this game is great.  It'...      5.0   \n",
      "662   Animal Crossing was always a sleeper hit on th...      5.0   \n",
      "663   I purchased Animal Crossing about a week ago f...      1.0   \n",
      "671   When I bought it I was hoping for Morrowind wi...      3.0   \n",
      "676   I wasn't sure about this game when I first hea...      5.0   \n",
      "678   The past few years have really shown that Nint...      4.0   \n",
      "692   Nintendo has presented us with a tough choice:...      5.0   \n",
      "704   Bought two of the blue ones.  Unboxed 'em, and...      1.0   \n",
      "\n",
      "                                                summary  helpful_total_review  \\\n",
      "32         Very useful; but get DVD-ROM version instead                 294.0   \n",
      "1637                         DIABLO III: THE PG VERSION                 139.0   \n",
      "1811                     Works fine for me in Windows 7                 159.0   \n",
      "3735  Misrepresented, complicated, and internet-depe...                 139.0   \n",
      "1                         Wrong information on web page                 149.0   \n",
      "2                       The biggest waste of money ever                 177.0   \n",
      "11                     overrated.......I have to agree.                 198.0   \n",
      "13         oh boy, this game is the most overhyped game                 121.0   \n",
      "14                                a game that is so bad                 163.0   \n",
      "19                           It's great!  I'm addicted.                 132.0   \n",
      "24                         Would rather eat candy canes                 123.0   \n",
      "37          Things they should tell you before you buy!                 111.0   \n",
      "42     The TV show is great just like the computer game                 293.0   \n",
      "45                                         disappointed                 467.0   \n",
      "50                                Great Star Trek game!                 101.0   \n",
      "72             Unlimited for old and new players alike!                 109.0   \n",
      "89                   A great expansion for a great game                 186.0   \n",
      "91      Harvest Moon: The Best Video Game in the World!                 111.0   \n",
      "96    A PRACTICAL PURCHASE: Transport all the Gamebo...                 174.0   \n",
      "111              A &quot;Must Have&quot; For Sims Fans!                 154.0   \n",
      "124                               How Good Games Go Bad                 103.0   \n",
      "126                   The Real Deal For Pokemon Crystal                 288.0   \n",
      "131   Hands down, the best current game for the Game...                 167.0   \n",
      "135   Just When You Think the Sims Can't Get Any Better                 227.0   \n",
      "166                       Great Gaming For Adult Gamers                 110.0   \n",
      "173                                          Incredible                 280.0   \n",
      "179                               This is great product                 224.0   \n",
      "184                              Warcraft 3 Beta Review                 146.0   \n",
      "187                                     All show, no go                 126.0   \n",
      "192                 A good program that could be better                 197.0   \n",
      "...                                                 ...                   ...   \n",
      "514                               BAD GAMEEEEEEEEEEEEEE                 120.0   \n",
      "519                   Rated &quot;E&quot; for Everyone?                 154.0   \n",
      "523                                 Lots of Improvement                 430.0   \n",
      "524                                 The Next Generation                 110.0   \n",
      "528     Excellent HOTAS system, great bang for the buck                 137.0   \n",
      "534                                         Great price                 414.0   \n",
      "540                                           SUPERB!!!                 134.0   \n",
      "546   A few issues to an otherwise fun and unique game!                 116.0   \n",
      "556          Great God Game with Bits of Creatures, AoE                 203.0   \n",
      "561   Yourself Fitness is a fantastic program for he...                 145.0   \n",
      "571                               Ah...Good Ol' HOMM 3!                 101.0   \n",
      "573          They still have the nerve to charge $50???                 105.0   \n",
      "575                                    Wasted Potential                 282.0   \n",
      "576   Broken promises & poor management is killing SWG.                 158.0   \n",
      "579            Great Fun and Scalable to Varying Levels                 148.0   \n",
      "585    An excellent game... even better than the first!                 199.0   \n",
      "595              Fun Ratchet World, Nice Co-Op Gameplay                 136.0   \n",
      "602         Same hack-slash routine..a bug..decent game                 135.0   \n",
      "609              This game is so much fun and adorable!                 101.0   \n",
      "632   Wii, inexpensive... like that's ever going to ...                 123.0   \n",
      "648   Awesome entertainment value, great for kids an...                 118.0   \n",
      "658           A great game only gets better and better!                 294.0   \n",
      "659           Addressing some complaints about the game                 114.0   \n",
      "662            Animal Crossing returns with a vengance!                 716.0   \n",
      "663                                           What the?                 109.0   \n",
      "671                          Fun but very disappointing                 135.0   \n",
      "676                             Sims 2 Works Well on DS                 180.0   \n",
      "678                                   The Party Returns                 182.0   \n",
      "692                                 Best handheld ever!                 123.0   \n",
      "704               Utter crap from one of the suppliers.                 124.0   \n",
      "\n",
      "      helpful_percent  text_length   price  rank_values        ...         \\\n",
      "32               0.96        515.0   37.99        14531        ...          \n",
      "1637             0.96       5460.0   19.99         1124        ...          \n",
      "1811             0.99       2261.0   24.89          445        ...          \n",
      "3735             0.82       3228.0   29.99         1756        ...          \n",
      "1                0.03        453.0   39.99          944        ...          \n",
      "2                0.05        853.0   39.99          944        ...          \n",
      "11               0.49       1209.0  179.99         2107        ...          \n",
      "13               0.49        394.0  179.99         2107        ...          \n",
      "14               0.49        898.0  179.99         2107        ...          \n",
      "19               0.98        699.0    7.47        23142        ...          \n",
      "24               0.02        515.0   19.99         2899        ...          \n",
      "37               0.97        324.0   67.99        22113        ...          \n",
      "42               0.16         65.0    3.00        12831        ...          \n",
      "45               0.97        335.0    3.00        12831        ...          \n",
      "50               0.56        190.0   39.99        17214        ...          \n",
      "72               0.94       1026.0   29.95         9337        ...          \n",
      "89               0.92       1270.0   11.49        12538        ...          \n",
      "91               0.88       1213.0   75.00         6759        ...          \n",
      "96               0.94        442.0   61.46        63277        ...          \n",
      "111              0.99       4157.0  124.99         8536        ...          \n",
      "124              0.02        452.0   19.99         7232        ...          \n",
      "126              0.91       2675.0  424.24         1459        ...          \n",
      "131              0.92       1270.0   99.95         2083        ...          \n",
      "135              0.99       5492.0   14.99         8953        ...          \n",
      "166              0.95       1131.0   19.99         2749        ...          \n",
      "173              0.79        823.0   44.97          645        ...          \n",
      "179              0.96        907.0   36.80        14959        ...          \n",
      "184              0.90       2991.0   29.99         6337        ...          \n",
      "187              0.90       1580.0   44.88       131442        ...          \n",
      "192              0.98       4480.0   89.99        19224        ...          \n",
      "...               ...          ...     ...          ...        ...          \n",
      "514              0.03        144.0   29.99         3244        ...          \n",
      "519              0.09        404.0   29.99         3244        ...          \n",
      "523              0.98       5088.0    9.99         8724        ...          \n",
      "524              0.95       4668.0   24.93        52924        ...          \n",
      "528              0.91       2762.0  129.95       131442        ...          \n",
      "534              0.81        611.0   58.87          593        ...          \n",
      "540              0.93       1858.0   12.99         5498        ...          \n",
      "546              0.97       3257.0   19.99        20884        ...          \n",
      "556              0.94       3155.0   30.98         1706        ...          \n",
      "561              0.99       2830.0   29.68        29854        ...          \n",
      "571              0.94        403.0   10.99        13372        ...          \n",
      "573              0.22       1094.0   69.95         3430        ...          \n",
      "575              0.88       5024.0   49.95        10583        ...          \n",
      "576              0.86       5772.0   49.95        10583        ...          \n",
      "579              0.97        866.0   84.60         8362        ...          \n",
      "585              0.93       2099.0   52.50          435        ...          \n",
      "595              0.95       2191.0   19.99         3590        ...          \n",
      "602              0.88       2780.0    4.51        14979        ...          \n",
      "609              0.89       1068.0   33.93         4975        ...          \n",
      "632              0.33       2457.0   29.99          360        ...          \n",
      "648              0.97       1484.0  103.99        20276        ...          \n",
      "658              1.00       3692.0   92.66         8696        ...          \n",
      "659              0.96       1681.0   49.99        22651        ...          \n",
      "662              0.95       4694.0   48.87         1769        ...          \n",
      "663              0.35       1008.0   48.87         1769        ...          \n",
      "671              0.78       2968.0   19.99         6890        ...          \n",
      "676              0.94       1182.0  169.90         6123        ...          \n",
      "678              0.88       2433.0   69.75         4532        ...          \n",
      "692              0.89        894.0    1.69         4324        ...          \n",
      "704              0.97        332.0   17.91         4298        ...          \n",
      "\n",
      "      percent_GROUP_1  percent_GROUP_2  percent_GROUP_3  percent_GROUP_4  \\\n",
      "32           0.011743         0.007881         0.000000         0.003414   \n",
      "1637         0.046771         0.000000         0.000000         0.000000   \n",
      "1811         0.026335         0.010290         0.000000         0.039473   \n",
      "3735         0.013123         0.013349         0.000000         0.043483   \n",
      "1            0.031799         0.000000         0.013484         0.021646   \n",
      "2            0.040191         0.000000         0.000000         0.000000   \n",
      "11           0.072274         0.000000         0.001915         0.009517   \n",
      "13           0.065215         0.000000         0.000000         0.011278   \n",
      "14           0.053819         0.000000         0.000000         0.000000   \n",
      "19           0.032297         0.000000         0.000000         0.007957   \n",
      "24           0.020575         0.000000         0.005871         0.000000   \n",
      "37           0.017592         0.005213         0.000000         0.016570   \n",
      "42           0.032415         0.001453         0.007930         0.013497   \n",
      "45           0.031706         0.001384         0.000000         0.001030   \n",
      "50           0.057923         0.000000         0.000000         0.005841   \n",
      "72           0.022780         0.000000         0.000000         0.016962   \n",
      "89           0.026146         0.000000         0.000000         0.000000   \n",
      "91           0.055848         0.000000         0.000000         0.009389   \n",
      "96           0.015149         0.000000         0.000000         0.000000   \n",
      "111          0.023442         0.000000         0.000000         0.000000   \n",
      "124          0.044032         0.000000         0.000000         0.008113   \n",
      "126          0.037495         0.000000         0.000000         0.000000   \n",
      "131          0.032198         0.004708         0.000000         0.014346   \n",
      "135          0.016515         0.000000         0.000000         0.000000   \n",
      "166          0.057353         0.000000         0.000000         0.004109   \n",
      "173          0.051642         0.000000         0.000000         0.000000   \n",
      "179          0.029654         0.003430         0.000000         0.000000   \n",
      "184          0.056059         0.000000         0.000000         0.000000   \n",
      "187          0.022153         0.003107         0.000000         0.000000   \n",
      "192          0.051950         0.000000         0.000000         0.000000   \n",
      "...               ...              ...              ...              ...   \n",
      "514          0.057806         0.000000         0.000000         0.042583   \n",
      "519          0.026921         0.000000         0.000000         0.000776   \n",
      "523          0.028951         0.000000         0.000000         0.000000   \n",
      "524          0.044198         0.000000         0.000000         0.000000   \n",
      "528          0.026651         0.000000         0.000000         0.000000   \n",
      "534          0.015813         0.007183         0.000000         0.015324   \n",
      "540          0.051846         0.002194         0.000000         0.000000   \n",
      "546          0.068061         0.000000         0.000000         0.000000   \n",
      "556          0.054219         0.000000         0.000000         0.000583   \n",
      "561          0.007797         0.000000         0.000000         0.000000   \n",
      "571          0.020326         0.005345         0.000000         0.000773   \n",
      "573          0.049926         0.021683         0.000000         0.000000   \n",
      "575          0.074632         0.000000         0.000000         0.000000   \n",
      "576          0.044241         0.005794         0.000000         0.004638   \n",
      "579          0.039870         0.000000         0.000000         0.000000   \n",
      "585          0.043876         0.000000         0.000000         0.000000   \n",
      "595          0.058977         0.000000         0.000000         0.000000   \n",
      "602          0.053229         0.000000         0.000000         0.000528   \n",
      "609          0.029349         0.000000         0.000000         0.000000   \n",
      "632          0.025411         0.067893         0.101855         0.015295   \n",
      "648          0.052219         0.000000         0.000000         0.003533   \n",
      "658          0.031721         0.000000         0.000000         0.000000   \n",
      "659          0.060145         0.001694         0.000000         0.000000   \n",
      "662          0.055041         0.000000         0.002901         0.000000   \n",
      "663          0.051402         0.000000         0.000000         0.001351   \n",
      "671          0.077292         0.000000         0.000000         0.000000   \n",
      "676          0.072345         0.000191         0.002913         0.014055   \n",
      "678          0.038073         0.000000         0.005504         0.001368   \n",
      "692          0.029601         0.004314         0.001149         0.000000   \n",
      "704          0.003742         0.000000         0.000000         0.012481   \n",
      "\n",
      "      percent_GROUP_5  percent_GROUP_6  percent_GROUP_7  percent_GROUP_8  \\\n",
      "32           0.000000         0.000000         0.000000         0.000000   \n",
      "1637         0.000000         0.000000         0.000000         0.000000   \n",
      "1811         0.000000         0.000000         0.000000         0.004046   \n",
      "3735         0.002690         0.000000         0.000000         0.000000   \n",
      "1            0.000000         0.014966         0.000000         0.000000   \n",
      "2            0.000000         0.000000         0.000000         0.000000   \n",
      "11           0.000000         0.000000         0.000000         0.000000   \n",
      "13           0.000000         0.000000         0.000000         0.000000   \n",
      "14           0.000000         0.000000         0.000000         0.000000   \n",
      "19           0.000000         0.000000         0.000000         0.008671   \n",
      "24           0.000000         0.000000         0.000000         0.000000   \n",
      "37           0.000000         0.000000         0.000000         0.000000   \n",
      "42           0.000000         0.000000         0.000000         0.000000   \n",
      "45           0.000000         0.000000         0.000000         0.000000   \n",
      "50           0.000000         0.000000         0.000000         0.000000   \n",
      "72           0.000000         0.000000         0.004716         0.000000   \n",
      "89           0.000000         0.000000         0.015856         0.000000   \n",
      "91           0.000000         0.000000         0.000000         0.000000   \n",
      "96           0.000000         0.000000         0.000000         0.010193   \n",
      "111          0.000000         0.000000         0.207150         0.000000   \n",
      "124          0.000000         0.000000         0.000000         0.000000   \n",
      "126          0.000000         0.000000         0.007323         0.000000   \n",
      "131          0.000000         0.162184         0.000000         0.000000   \n",
      "135          0.000000         0.000000         0.221873         0.000000   \n",
      "166          0.000000         0.000000         0.000000         0.000000   \n",
      "173          0.000000         0.000000         0.000000         0.000000   \n",
      "179          0.000000         0.000000         0.000000         0.006643   \n",
      "184          0.000000         0.000000         0.000000         0.000000   \n",
      "187          0.000000         0.000000         0.000000         0.008495   \n",
      "192          0.006184         0.000000         0.000000         0.000000   \n",
      "...               ...              ...              ...              ...   \n",
      "514          0.000000         0.000000         0.000000         0.000000   \n",
      "519          0.000000         0.000000         0.000000         0.000000   \n",
      "523          0.000000         0.000000         0.000000         0.000000   \n",
      "524          0.000000         0.000000         0.007364         0.000000   \n",
      "528          0.000000         0.000000         0.011749         0.025054   \n",
      "534          0.000000         0.000000         0.000000         0.000000   \n",
      "540          0.000000         0.007691         0.000000         0.000000   \n",
      "546          0.000000         0.007638         0.000000         0.000000   \n",
      "556          0.000000         0.000000         0.000000         0.000000   \n",
      "561          0.205577         0.000000         0.000000         0.000000   \n",
      "571          0.000000         0.000000         0.000000         0.000000   \n",
      "573          0.000000         0.000000         0.000000         0.000000   \n",
      "575          0.000000         0.000000         0.000000         0.000000   \n",
      "576          0.000000         0.000000         0.000000         0.000000   \n",
      "579          0.000000         0.000000         0.037700         0.000000   \n",
      "585          0.000000         0.000000         0.000000         0.000000   \n",
      "595          0.000000         0.000000         0.000000         0.000000   \n",
      "602          0.000000         0.000000         0.000000         0.000000   \n",
      "609          0.000000         0.000000         0.000000         0.000000   \n",
      "632          0.004202         0.000000         0.000000         0.024719   \n",
      "648          0.000000         0.000561         0.000000         0.009826   \n",
      "658          0.000000         0.000000         0.000000         0.000000   \n",
      "659          0.000000         0.030839         0.000000         0.000000   \n",
      "662          0.000000         0.021140         0.005058         0.000000   \n",
      "663          0.000000         0.000000         0.000000         0.000000   \n",
      "671          0.000000         0.000000         0.000000         0.000000   \n",
      "676          0.000000         0.000761         0.033937         0.000000   \n",
      "678          0.000000         0.196070         0.000000         0.000000   \n",
      "692          0.000000         0.005671         0.002804         0.000000   \n",
      "704          0.000000         0.000000         0.000000         0.086710   \n",
      "\n",
      "      percent_GROUP_9  percent_GROUP_10  \n",
      "32           0.000000          0.000000  \n",
      "1637         0.000000          0.000000  \n",
      "1811         0.000000          0.000000  \n",
      "3735         0.000000          0.000000  \n",
      "1            0.000000          0.000000  \n",
      "2            0.000000          0.000000  \n",
      "11           0.000000          0.000000  \n",
      "13           0.000000          0.000000  \n",
      "14           0.000000          0.000000  \n",
      "19           0.000000          0.000000  \n",
      "24           0.000000          0.000000  \n",
      "37           0.000000          0.000000  \n",
      "42           0.000000          0.000000  \n",
      "45           0.000000          0.000000  \n",
      "50           0.000000          0.000000  \n",
      "72           0.000000          0.000000  \n",
      "89           0.000000          0.000000  \n",
      "91           0.000000          0.000000  \n",
      "96           0.000000          0.000000  \n",
      "111          0.091477          0.000000  \n",
      "124          0.000000          0.000000  \n",
      "126          0.000000          0.000000  \n",
      "131          0.000000          0.000000  \n",
      "135          0.000000          0.000000  \n",
      "166          0.000000          0.000000  \n",
      "173          0.000000          0.000000  \n",
      "179          0.000000          0.000000  \n",
      "184          0.000000          0.000000  \n",
      "187          0.000000          0.000000  \n",
      "192          0.000000          0.000000  \n",
      "...               ...               ...  \n",
      "514          0.000000          0.000000  \n",
      "519          0.000000          0.000000  \n",
      "523          0.000000          0.000000  \n",
      "524          0.000000          0.000000  \n",
      "528          0.000474          0.000000  \n",
      "534          0.000000          0.000000  \n",
      "540          0.000000          0.000000  \n",
      "546          0.026163          0.000000  \n",
      "556          0.000000          0.000000  \n",
      "561          0.000000          0.000000  \n",
      "571          0.000000          0.000000  \n",
      "573          0.000000          0.000000  \n",
      "575          0.000000          0.000000  \n",
      "576          0.000000          0.000000  \n",
      "579          0.000000          0.000000  \n",
      "585          0.000000          0.000000  \n",
      "595          0.000000          0.000000  \n",
      "602          0.000000          0.000000  \n",
      "609          0.000000          0.000000  \n",
      "632          0.000000          0.000847  \n",
      "648          0.000000          0.000000  \n",
      "658          0.221047          0.000000  \n",
      "659          0.086222          0.000000  \n",
      "662          0.000000          0.000000  \n",
      "663          0.000000          0.000000  \n",
      "671          0.000000          0.000000  \n",
      "676          0.000000          0.000000  \n",
      "678          0.000000          0.000000  \n",
      "692          0.000000          0.000000  \n",
      "704          0.000000          0.000000  \n",
      "\n",
      "[100 rows x 93 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 47.3345038891 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_test_new = dp.df_prep_test(df_test, tfidf_model,NMF_model, TFIDF_feat_model, 'Video Games')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking out columns that are all 0\n",
    "\n",
    "Before running ensemble method algorithm,  \n",
    "I want to identify any columns that only have value 0 in my training set.  \n",
    "These columns will not affect my prediction results and should be removed to reduce dimensionality of my model.  \n",
    "  \n",
    "In order to do this, I identify these columns from my training dataset first and  \n",
    "removed them from both training and test dataset.  \n",
    "Test dataset may contain columns with zero values that are different from training set.  \n",
    "However, these will be kept in our dataset because we only care about columns with zeros  \n",
    "in our training set during our model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Drums', u'Skins', u'Fire TV'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "zero_columns_train = df_train_new.columns[(df_train_new == 0).all()]\n",
    "print zero_columns_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_new = df_train_new.drop(zero_columns_train, 1)\n",
    "df_test_new = df_test_new.drop(zero_columns_train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making XGboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing labels for prediction modeling\n",
    "Before running prediction model with XGboost, we want to create labels for our model. \n",
    "For this prediction model, I created a feature called 'helpful_percent'  \n",
    "which indicates percentage of user voted review as helpful review.\n",
    "I want to split this feature into two groups to make my prediction model binary classifier.\n",
    "\n",
    "the function **label_prep** is a function that prepares label feature.\n",
    "It also prints out total number of reviews that belongs LOW and HIGH label class in your data frame.  \n",
    "\n",
    "input:\n",
    "**dp.label_prep( a , b )**\n",
    "a = train or test dataframe.  \n",
    "b = float value that splits label.  \n",
    "the label feature has value HIGH or LOW.  \n",
    "If the 'helpful_precent' is higher than the value of b, label == HIGH.  \n",
    "If the 'helpful_precent' is lower than the value of b, label == LOW.  \n",
    "I usually set this values around 0.75 to 0.9.  \n",
    "What I care most here is predicting HIGH correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  \n",
      "highly helpful count: 2232\n",
      "not helpful count: 1366\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_pickle(\"./preped_videogame_1000_df.pkl\")\n",
    "print \"training data:  \"\n",
    "df_train_new = dp.label_prep(df_train_new, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data: \n",
      "highly helpful count: 394\n",
      "not helpful count: 242\n"
     ]
    }
   ],
   "source": [
    "print \"test data: \"\n",
    "df_test_new = dp.label_prep(df_test_new,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjusting number of data in training and test sets\n",
    "Another step you want to apply before applying XGboost.\n",
    "Since training and test dataframe most likely have unbalanced label classes (HIGH and LOW),  \n",
    "we want to balance label classes by randomly select set numbers of reviews from each class.  \n",
    "From the print statement of previous step (label_prep), you know how many reviews exist in both HIGH and LOW label class.  \n",
    "Select a number so that the number of reviews for HIGH and LOW are about same.  \n",
    "  \n",
    "For example:  \n",
    "if label_prep prints out:\n",
    "  \n",
    "highly helpful count: 34290  \n",
    "not helpful count: 25345  \n",
    "\n",
    "I would select number 25000 (if you really want to increase number of reviews, 27500?).  \n",
    "  \n",
    "input:  \n",
    "**df_for_XGBOOST**( a , b )  \n",
    "a = dataframe  \n",
    "b = maximum number of reviews in each label class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y, df_new_train = dp.df_for_XGBOOST(df_train_new,1400)\n",
    "test_X, test_y, df_new_test = dp.df_for_XGBOOST(df_test_new,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost model\n",
    "function **XGBOOSTING** creates XGboost model, fits training dataset and prints out test data accuracy score.\n",
    "\n",
    "inputs:\n",
    "**XGBOOSTING(train_X,test_X,train_y,test_y,xgb_para=[4000,0.15])**\n",
    "a = X train  \n",
    "b = X test  \n",
    "c = label train  \n",
    "d = label test  \n",
    "e = parameters for XGboost. xgb_para = [4000,0.15]  \n",
    "where xgb = XGB.XGBClassifier(n_estimators=xgb_para[0], learning_rate=xgb_para[1])  \n",
    "  \n",
    "  \n",
    "### Random Forest model\n",
    "function **r_forest** runs random frest insted of XGBoost. Only difference is the parameter.  \n",
    "  \n",
    "inputs:  \n",
    "**r_forest(train_X,test_X,train_y,test_y,parameter=[1000,'auto',none])**  \n",
    "parameter = [1000,'auto',None]  \n",
    "where rf = RandomForestClassifier(n_estimators=parameter[0], max_features = parameter[1], max_depth=parameter[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 21.9913899899 seconds ---\n",
      "RF score: 86.32%\n"
     ]
    }
   ],
   "source": [
    "#model = dp.XGBOOSTING(train_X,test_X,train_y,test_y,xgb_para=[4000,0.15])\n",
    "model = dp.r_forest(train_X,test_X,train_y,test_y, [1000,50,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost stats (Important Features and confusion matrix)\n",
    "following function prints out Top15 important features of the prediction model,  \n",
    "confusion matrix that is in the markdown format table,  \n",
    "LOW prediction rate and HIGH prediction rate.  \n",
    "\n",
    "input:  \n",
    "**xgb_stats**( a , b , c , d )  \n",
    "a = XGboost model  \n",
    "b = dataframe (an output of df_for_XGBOOST function)  \n",
    "c = test X data  \n",
    "d = test label data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **TOP15 Important Features**  \n",
      "leave : 0.672773805517%  \n",
      "new : 0.699647532105%  \n",
      "Accessories : 0.818463379415%  \n",
      "percent_GROUP_1 : 0.84741773711%  \n",
      "feature : 0.853787932902%  \n",
      "enjoy : 0.907305699071%  \n",
      "percent_GROUP_4 : 0.960251319721%  \n",
      "sort : 1.10448132333%  \n",
      "die : 1.12857298946%  \n",
      "surprise : 1.22353217363%  \n",
      "price : 1.42303649998%  \n",
      "gamecube : 1.61354571494%  \n",
      "rank_values : 3.45245920816%  \n",
      "text_length : 6.00004820027%  \n",
      "overall : 8.50613117299%  \n",
      "\n",
      "\n",
      " |                 |       NOT HELPFUL TRUE       |        HIGHLY HELPFUL TRUE        |\n",
      "     |:--------------: | :-------------------:|:-----------------------:|\n",
      "     |       NOT HELPFUL PRED     |        205.0        |           50.0        |\n",
      "     |        HIGHLY HELPFUL PRED     |        37.0        |           344.0        |\n",
      "\n",
      "LOW prediction rate: 84.71%\n",
      "HIGH prediction rate: 87.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp.xgb_stats(model,df_new_test,test_X,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### XGB RESULTS with 75% threshold\n",
    "  \n",
    "n_estimators=2000,learning_rate=0.1,subsample =0.9, max_depth =25, gamma = 0.3, colsample_bytree =0.8, reg_alpha = 0.01  \n",
    "Computation: 2025.37363505 seconds  \n",
    "score: 77.24%  \n",
    " **TOP15 Important Features**  \n",
    "percent_GROUP_5 : 1.06626739725%  \n",
    "overall : 1.06887128204%  \n",
    "plan : 1.08449421823%  \n",
    "tank : 1.10662672669%  \n",
    "canister : 1.14698605612%  \n",
    "unit : 1.20166642591%  \n",
    "bring : 1.30321569741%  \n",
    "percent_GROUP_3 : 1.35008459911%  \n",
    "firm : 1.64041146636%  \n",
    "percent_GROUP_8 : 1.82528309524%  \n",
    "percent_GROUP_1 : 2.2015362978%  \n",
    "text_length : 3.62322628498%  \n",
    "percent_GROUP_10 : 3.6336414516%  \n",
    "price : 3.9552140981%  \n",
    "rank_values : 4.77411784232%  \n",
    "\n",
    "\n",
    " |                 |       NOT HELPFUL TRUE       |        HIGHLY HELPFUL TRUE        |  \n",
    "     |:--------------: | :-------------------:|:-----------------------:|  \n",
    "     |       NOT HELPFUL PRED     |        1426.0        |           473.0        |  \n",
    "     |        HIGHLY HELPFUL PRED     |        397.0        |           1527.0        |  \n",
    "  \n",
    "LOW prediction rate: 78.22%  \n",
    "HIGH prediction rate: 76.35%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Random Forest result with 75% threshold\n",
    "  \n",
    "1000,50,None  \n",
    "224 sec.  \n",
    "Overall accuracy: 76.82%  \n",
    "**TOP15 Important Features**  \n",
    "bring : 0.583035827847%  \n",
    "tank : 0.595895823154%  \n",
    "canister : 0.630979208038%  \n",
    "whatev : 0.656540650705%  \n",
    "percent_GROUP_3 : 0.672279437706%  \n",
    "percent_GROUP_8 : 0.802946732109%  \n",
    "percent_GROUP_10 : 0.885243888022%  \n",
    "percent_GROUP_1 : 0.888022789254%  \n",
    "firm : 0.905402625438%  \n",
    "rank_values : 1.02694375671%  \n",
    "loud : 1.09024337464%  \n",
    "price : 1.33508923173%  \n",
    "main : 1.36355712797%  \n",
    "text_length : 2.80382163106%  \n",
    "overall : 8.20653149646%  \n",
    "\n",
    "\n",
    " |                 |       NOT HELPFUL TRUE       |        HIGHLY HELPFUL TRUE        |  \n",
    "     |:--------------: | :-------------------:|:-----------------------:|  \n",
    "     |       NOT HELPFUL PRED     |        1492.0        |           555.0        |  \n",
    "     |        HIGHLY HELPFUL PRED     |        331.0        |           1445.0        |  \n",
    "\n",
    "LOW prediction rate: 81.84%  \n",
    "HIGH prediction rate: 72.25%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gridsearchcv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'min_child_weight':range(1,6,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation: 24289.086812 seconds  \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gsearch = GridSearchCV(estimator = XGB.XGBClassifier( learning_rate =0.1, n_estimators=2000, max_depth=25,\n",
    " min_child_weight=1, subsample=0.9), param_grid = param_test1, cv=5)\n",
    "gsearch.fit(train_X,train_y)\n",
    "gsearch.best_score_, gsearch.best_params_, gsearch.best_estimator_\n",
    "print(\"Computation: %s seconds  \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'min_child_weight': 1},\n",
       " XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "        gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=25,\n",
       "        min_child_weight=1, missing=None, n_estimators=2000, nthread=-1,\n",
       "        objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "        scale_pos_weight=1, seed=0, silent=True, subsample=0.9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.best_params_, gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
